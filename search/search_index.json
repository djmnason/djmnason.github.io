{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data science portfolio Welcome to Daniel Nason's data science portfolio! My career journey While I did not start my career as a data professional, after returning to school to get my master's degree in Statistics I was able to successfully change my career path into data. I am a data scientist with a passion for learning new tools and methodologies, as well as collecting, processing, exploring, and analyzing data and extracting insights to make informed decisions. Projects Statistical Genomics and High Dimensional Inference This was a data science project using R to perform analyses of high-dimensional data. Employee Attrition Prediction This was a data science project that used R and Python to build classification models on data from a Kaggle competition to predict an employee's likelihood of attrition. Program Language Usage Forecasting This was a data science project where I collaborated with my classmates and used R to build time-series models, including ARIMA and VARMAX models, for inference and forecasting of existing programming languages based on their StackOverflow frequency. Capital Bikeshare Prediction Analysis This was a data science project originally from my master's program that I reworked using Python to collect, process, explore, and model availability of bikes at different rideshare stations in and around Washington, D.C. Masters's Program Capstone - PHIGHT COVID This was my master's program capstone project that built off previously completed work to explore the impact of K-12 teaching posture on COVID-19 disease transmission and outcomes. Deal or No Deal Banker Analysis and Simulator This project collects and explores how a the banker in the popular game show Deal or No Deal offers deals to players. In addition to modeling the banker's behavior based on the status of a player's board, it also develops a simple simulator that allows you to play the game with simulated offers generated based on historical game data.","title":"Home"},{"location":"#data-science-portfolio","text":"Welcome to Daniel Nason's data science portfolio!","title":"Data science portfolio"},{"location":"#my-career-journey","text":"While I did not start my career as a data professional, after returning to school to get my master's degree in Statistics I was able to successfully change my career path into data. I am a data scientist with a passion for learning new tools and methodologies, as well as collecting, processing, exploring, and analyzing data and extracting insights to make informed decisions.","title":"My career journey"},{"location":"#projects","text":"Statistical Genomics and High Dimensional Inference This was a data science project using R to perform analyses of high-dimensional data. Employee Attrition Prediction This was a data science project that used R and Python to build classification models on data from a Kaggle competition to predict an employee's likelihood of attrition. Program Language Usage Forecasting This was a data science project where I collaborated with my classmates and used R to build time-series models, including ARIMA and VARMAX models, for inference and forecasting of existing programming languages based on their StackOverflow frequency. Capital Bikeshare Prediction Analysis This was a data science project originally from my master's program that I reworked using Python to collect, process, explore, and model availability of bikes at different rideshare stations in and around Washington, D.C. Masters's Program Capstone - PHIGHT COVID This was my master's program capstone project that built off previously completed work to explore the impact of K-12 teaching posture on COVID-19 disease transmission and outcomes. Deal or No Deal Banker Analysis and Simulator This project collects and explores how a the banker in the popular game show Deal or No Deal offers deals to players. In addition to modeling the banker's behavior based on the status of a player's board, it also develops a simple simulator that allows you to play the game with simulated offers generated based on historical game data.","title":"Projects"},{"location":"Capital_Bikeshare_Analysis/","text":"Capitol Bikeshare This repository contains my code and data used for predicting bike availability in Washington D.C. using the publicly available Capital Bikeshare dataset. This was originally a project for my Master's program that I then completed on my own time and increasing the scale of the project. The final presentation can be found here . Data collection The processing notebook details the code that was used to collect the data from publicly available sources Exploration The exploration notebook shows the steps I took to explore the data and some written analysis about the decisions made based on this exploration. Activity Over time Geographically arrivals departures Modeling Based on the results of the exploration, I tested and evaluated multiple machine learning models using predictive modeling best practices including train-test splits, hyperparameter tuning, and cross-validation. The results of the modeling can be found here . Variable Relationships XGBoost Modeling Results Errors","title":"Capital Bikeshare Prediction Analysis"},{"location":"Capital_Bikeshare_Analysis/#capitol-bikeshare","text":"This repository contains my code and data used for predicting bike availability in Washington D.C. using the publicly available Capital Bikeshare dataset. This was originally a project for my Master's program that I then completed on my own time and increasing the scale of the project. The final presentation can be found here .","title":"Capitol Bikeshare"},{"location":"Capital_Bikeshare_Analysis/#data-collection","text":"The processing notebook details the code that was used to collect the data from publicly available sources","title":"Data collection"},{"location":"Capital_Bikeshare_Analysis/#exploration","text":"The exploration notebook shows the steps I took to explore the data and some written analysis about the decisions made based on this exploration.","title":"Exploration"},{"location":"Capital_Bikeshare_Analysis/#activity","text":"","title":"Activity"},{"location":"Capital_Bikeshare_Analysis/#over-time","text":"","title":"Over time"},{"location":"Capital_Bikeshare_Analysis/#geographically","text":"","title":"Geographically"},{"location":"Capital_Bikeshare_Analysis/#arrivals","text":"","title":"arrivals"},{"location":"Capital_Bikeshare_Analysis/#departures","text":"","title":"departures"},{"location":"Capital_Bikeshare_Analysis/#modeling","text":"Based on the results of the exploration, I tested and evaluated multiple machine learning models using predictive modeling best practices including train-test splits, hyperparameter tuning, and cross-validation. The results of the modeling can be found here .","title":"Modeling"},{"location":"Capital_Bikeshare_Analysis/#variable-relationships","text":"","title":"Variable Relationships"},{"location":"Capital_Bikeshare_Analysis/#xgboost-modeling-results","text":"","title":"XGBoost Modeling Results"},{"location":"Capital_Bikeshare_Analysis/#errors","text":"","title":"Errors"},{"location":"Capstone_PHIGHT_COVID/","text":"PHIGHT COVID Capstone Project: MSP Class of 2022 Authors: Daniel Nason, Wei-Yu Tseng, Julia Keating, Ziyan Wang, Hongsheng Xie Advisor: Valeria Ventura, Carnegie Mellon University Department of Statistics and Data Science Collaborators: PHIGHT COVID , Lakdawala Lab, University of Pittsburgh Deliverables Check out our presentation and final report for a more in-depth explanation. Introduction Building off of the work previous completed by the MSP Class of 2021 , we explore the impact of teaching posture on disease transmission. Specifically, we attempt to validate the results from the article based on the project's findings using the methods in Cori et al. 2013 . Methods The effective reproductive report R t outlined in Cori et al. 2013 estimates the ratio of the number of new infections generated at a given time to the total infectiousness of infected individuals at the same time. Note that infections need to be inferred through an observable time-series (i.e. cases, deaths) collected based on the disease, which requires a deconvolution in order to estimate R t . Miller et al. 2020 detail this process in Figure 1 : Due to the volatility in the cases time series, we focus on the deaths time-series despite its smaller sample size due to the reliability of the data collected relative to cases of testing positive. Findings We compare the change in R t following the start of the school year to determine the impact of teaching posture (i.e. on-premises, online-only, or hybrid) for K-12 students on COVID-19 disease transmission. We split the semester up into two time-periods, as detailed in the following visual: If there was a relationship between disease transmission and whether students were in school, we would expect to see a strong positive relationship between teaching posture and R t for on-premises teaching compared to online-only teaching. However, there does not appear to be any difference in the relationship between teaching posture and the change in R t during the time periods of interest, as seen in the following visuals:","title":"Master's Program Capstone - PHIGHT COVID"},{"location":"Capstone_PHIGHT_COVID/#phight-covid-capstone-project-msp-class-of-2022","text":"Authors: Daniel Nason, Wei-Yu Tseng, Julia Keating, Ziyan Wang, Hongsheng Xie Advisor: Valeria Ventura, Carnegie Mellon University Department of Statistics and Data Science Collaborators: PHIGHT COVID , Lakdawala Lab, University of Pittsburgh","title":"PHIGHT COVID Capstone Project: MSP Class of 2022"},{"location":"Capstone_PHIGHT_COVID/#deliverables","text":"Check out our presentation and final report for a more in-depth explanation.","title":"Deliverables"},{"location":"Capstone_PHIGHT_COVID/#introduction","text":"Building off of the work previous completed by the MSP Class of 2021 , we explore the impact of teaching posture on disease transmission. Specifically, we attempt to validate the results from the article based on the project's findings using the methods in Cori et al. 2013 .","title":"Introduction"},{"location":"Capstone_PHIGHT_COVID/#methods","text":"The effective reproductive report R t outlined in Cori et al. 2013 estimates the ratio of the number of new infections generated at a given time to the total infectiousness of infected individuals at the same time. Note that infections need to be inferred through an observable time-series (i.e. cases, deaths) collected based on the disease, which requires a deconvolution in order to estimate R t . Miller et al. 2020 detail this process in Figure 1 : Due to the volatility in the cases time series, we focus on the deaths time-series despite its smaller sample size due to the reliability of the data collected relative to cases of testing positive.","title":"Methods"},{"location":"Capstone_PHIGHT_COVID/#findings","text":"We compare the change in R t following the start of the school year to determine the impact of teaching posture (i.e. on-premises, online-only, or hybrid) for K-12 students on COVID-19 disease transmission. We split the semester up into two time-periods, as detailed in the following visual: If there was a relationship between disease transmission and whether students were in school, we would expect to see a strong positive relationship between teaching posture and R t for on-premises teaching compared to online-only teaching. However, there does not appear to be any difference in the relationship between teaching posture and the change in R t during the time periods of interest, as seen in the following visuals:","title":"Findings"},{"location":"DOND_Analysis/","text":"Deal or No Deal Analysis This repository contains my code and data used to analyze game data from the American version of the TV show Deal or No Deal that premiered on NBC in 2005. I also build a simple game simulator using a model created from the analysis. The project was motivated by my passion for the game and curiousity in investigating patterns that I saw watching the game show. Data Collection Unfortunately, there wasn't a publicly available database from the show that collects data at the game-round-round turn level, and transcripts of the show did not have great audio quality to scrape and structure into formatted data. Therefore, I watched and recorded data on multiple games from seasons 1 through 3 (as of 4/30/2024) in order to guarantee data quality for the analysis. I also scraped episode metadata from IMDB to supplement my collection process; this notebook generates the data . The Wikipedia page linked provides in-depth instructions on how the game is played, so I will comment on the rules I followed to collect the data. 1) The games data is collected at the game-round-round turn level in order to uniquely identify each decision made by the contestant during the game as well as interactions between them and the banker. 2) There are indicator columns for: 1) if the game ended (player made decision), 2) when the decision was made, and 3) the value of the contestant's original case selection. These are 1 if true and null otherwise. Note that 1) a single game could have multiple indicators with a value of 1, i.e. if the contestant takes a deal instead of playing to the end of the game, versus 2) and 3) only have a single row per game where they can take on a value of 1. 3) In the case that players do take a deal, depending on the state of the game (i.e. values left on the board) the host may continue to allow them to select cases. These can be found in the data as rows in the offer column that are not null when the game ended column indicator is 1. NOTE: in the case that the game ended AND the player had unopened cases AND the offer column is null in the data, do not assume that the contestant opened the cases in the order they appear in the data. That is, the game will usually reveal the values in each case at the end of the game, and I entered these based on the ascending order of the case NUMBERS (not values) for completeness. If you do decide to look at the data, please keep in mind that these are likely invalid rows. 4) There is exactly one row per game where the winnings column is not null, and it corresponds with the row where the decision_made indicator is 1. 5) There are techincally only 9 rounds in which the player can receive an offer from the banker, as you will see in the data. In the case that the player refuses every offer, they have the choice of choosing between their original selection and the one remaining case. To account for this in the data, I have encoded round 10 as the player staying with their own case, and round 11 as the player switching cases. In every game the rows associated with these rounds will have null values for offers. In all the games I watched, no one decided to switch to the last remaining case. 6) For simplicity in later analyses, I only collected data on games that have the 'standard' case values, which you can find here . I did not collect data on any special occassions where the board was different than usual or the case values could not be easily mapped back to their standard case value equivalents. With these rules I attempt to make a well-defined dataset that permit me more exploration and analysis of the data. I also collect demographic data on the contestants for completeness, although I do not explore the data any further with these variables. Exploration of the data While the rules of the data collection are well-defined, additional processing is required in order to manipulate it into a shape suitable for my analysis. I use this notebook to generate the processed data that is used for both data exploration and modeling. The exploration notebook goes into detail on the results of my EDA of the data and rationale for model selection, but I include some interesting visualizations based on my exploratory data analysis. Winnings Offers Modeling Based on the results of my exploration, I build multiple models in the modeling notebook and evaluate their performance based on out of sample data. The notebook goes more in-depth about model selection, although I ultimately decide to choose a linear model based on the exploration from the previous section as my preferred model of choice due to its interpretability and relatively strong performance compared to other machine learning models. The latex can be found here, and I also include goodness of fit visuals. Game simulator Finally, I finish the project by creating a simulator of the Deal or No Deal game that can be run in the terminal using the files within the game directory. I include a screenshot below of a player UI for the game. As described in the modeling notebook , I end up creating a model that randomly samples the game data rather than using the linear model due to the quirkiness of the predictions generated by the model in some edge cases (i.e. negative offers). Feel free to try it out on your own!","title":"Dealer or No Deal Banker Analysis"},{"location":"DOND_Analysis/#deal-or-no-deal-analysis","text":"This repository contains my code and data used to analyze game data from the American version of the TV show Deal or No Deal that premiered on NBC in 2005. I also build a simple game simulator using a model created from the analysis. The project was motivated by my passion for the game and curiousity in investigating patterns that I saw watching the game show.","title":"Deal or No Deal Analysis"},{"location":"DOND_Analysis/#data-collection","text":"Unfortunately, there wasn't a publicly available database from the show that collects data at the game-round-round turn level, and transcripts of the show did not have great audio quality to scrape and structure into formatted data. Therefore, I watched and recorded data on multiple games from seasons 1 through 3 (as of 4/30/2024) in order to guarantee data quality for the analysis. I also scraped episode metadata from IMDB to supplement my collection process; this notebook generates the data . The Wikipedia page linked provides in-depth instructions on how the game is played, so I will comment on the rules I followed to collect the data. 1) The games data is collected at the game-round-round turn level in order to uniquely identify each decision made by the contestant during the game as well as interactions between them and the banker. 2) There are indicator columns for: 1) if the game ended (player made decision), 2) when the decision was made, and 3) the value of the contestant's original case selection. These are 1 if true and null otherwise. Note that 1) a single game could have multiple indicators with a value of 1, i.e. if the contestant takes a deal instead of playing to the end of the game, versus 2) and 3) only have a single row per game where they can take on a value of 1. 3) In the case that players do take a deal, depending on the state of the game (i.e. values left on the board) the host may continue to allow them to select cases. These can be found in the data as rows in the offer column that are not null when the game ended column indicator is 1. NOTE: in the case that the game ended AND the player had unopened cases AND the offer column is null in the data, do not assume that the contestant opened the cases in the order they appear in the data. That is, the game will usually reveal the values in each case at the end of the game, and I entered these based on the ascending order of the case NUMBERS (not values) for completeness. If you do decide to look at the data, please keep in mind that these are likely invalid rows. 4) There is exactly one row per game where the winnings column is not null, and it corresponds with the row where the decision_made indicator is 1. 5) There are techincally only 9 rounds in which the player can receive an offer from the banker, as you will see in the data. In the case that the player refuses every offer, they have the choice of choosing between their original selection and the one remaining case. To account for this in the data, I have encoded round 10 as the player staying with their own case, and round 11 as the player switching cases. In every game the rows associated with these rounds will have null values for offers. In all the games I watched, no one decided to switch to the last remaining case. 6) For simplicity in later analyses, I only collected data on games that have the 'standard' case values, which you can find here . I did not collect data on any special occassions where the board was different than usual or the case values could not be easily mapped back to their standard case value equivalents. With these rules I attempt to make a well-defined dataset that permit me more exploration and analysis of the data. I also collect demographic data on the contestants for completeness, although I do not explore the data any further with these variables.","title":"Data Collection"},{"location":"DOND_Analysis/#exploration-of-the-data","text":"While the rules of the data collection are well-defined, additional processing is required in order to manipulate it into a shape suitable for my analysis. I use this notebook to generate the processed data that is used for both data exploration and modeling. The exploration notebook goes into detail on the results of my EDA of the data and rationale for model selection, but I include some interesting visualizations based on my exploratory data analysis.","title":"Exploration of the data"},{"location":"DOND_Analysis/#winnings","text":"","title":"Winnings"},{"location":"DOND_Analysis/#offers","text":"","title":"Offers"},{"location":"DOND_Analysis/#modeling","text":"Based on the results of my exploration, I build multiple models in the modeling notebook and evaluate their performance based on out of sample data. The notebook goes more in-depth about model selection, although I ultimately decide to choose a linear model based on the exploration from the previous section as my preferred model of choice due to its interpretability and relatively strong performance compared to other machine learning models. The latex can be found here, and I also include goodness of fit visuals.","title":"Modeling"},{"location":"DOND_Analysis/#game-simulator","text":"Finally, I finish the project by creating a simulator of the Deal or No Deal game that can be run in the terminal using the files within the game directory. I include a screenshot below of a player UI for the game. As described in the modeling notebook , I end up creating a model that randomly samples the game data rather than using the linear model due to the quirkiness of the predictions generated by the model in some edge cases (i.e. negative offers). Feel free to try it out on your own!","title":"Game simulator"},{"location":"Employee_Attrition_Prediction/","text":"Employee Attrition Prediction Project This project uses synthetic data from Kaggle to develop a classification model for the likelihood of an employee to leave the organization, tested on out-of-sample data. Data The data can be found via Kaggle or within the repo . Exploration Preliminary exploration shows that there is an imbalanced outcome variable. Interestingly as well, the data show no relationship between income and attrition, a non-intuitive relationship that likely resulted from the data being simulated. Given this information, we build supervised learning models to try and find the model with best performance on out-of-sample data. Results The results of the model fitting illustrate that logistic regression model performs best on out-of-sample data, so that is the model we prefer. Exploring the feature importances for prediction, we see that some themes emerge in employees more likely to attrit. These results could be valuable to HR to identify employees that fall within high-likelihood attrition categories to determine if intervention is worthwhile to prevent them from leaving the organization.","title":"Employee Attrition Prediction"},{"location":"Employee_Attrition_Prediction/#employee-attrition-prediction-project","text":"This project uses synthetic data from Kaggle to develop a classification model for the likelihood of an employee to leave the organization, tested on out-of-sample data.","title":"Employee Attrition Prediction Project"},{"location":"Employee_Attrition_Prediction/#data","text":"The data can be found via Kaggle or within the repo .","title":"Data"},{"location":"Employee_Attrition_Prediction/#exploration","text":"Preliminary exploration shows that there is an imbalanced outcome variable. Interestingly as well, the data show no relationship between income and attrition, a non-intuitive relationship that likely resulted from the data being simulated. Given this information, we build supervised learning models to try and find the model with best performance on out-of-sample data.","title":"Exploration"},{"location":"Employee_Attrition_Prediction/#results","text":"The results of the model fitting illustrate that logistic regression model performs best on out-of-sample data, so that is the model we prefer. Exploring the feature importances for prediction, we see that some themes emerge in employees more likely to attrit. These results could be valuable to HR to identify employees that fall within high-likelihood attrition categories to determine if intervention is worthwhile to prevent them from leaving the organization.","title":"Results"},{"location":"Programming_Language_Forecasting/","text":"Time Series and Experimental Design Authors: Alana Willis, Clare Cruz, Daniel Nason, Megan Christy Time Series Analysis Final Project The presentation can be found here , as well as the written report . Project Background Data The data can be found in this repo or on Kaggle . Findings In the dataset, python has grown the most in popularity of asking StackOverflow questions, followed by R and then various statistical and machine learning topics. SARIMA Models Model fits were chosen based on residual diagnostics. While both models fit the data relatively well and project a growing trend, additional modeling is needed to determine which variables are correlated with Python and R. VAR Models Since the predictor variables of interest show a strong linear correlation with R and Python, we fit a VAR model to attempt to understand these relationships. The models show a good fit to the variables, although slightly less of a good fit compared to the SARIMA models. Interestingly, the results show that the statistical and machine learning topics are not statistically significant predictors of Python, while only Classification and Cluster Analysis are statistically significant for R. Conclusion","title":"Programming Language Forecasting"},{"location":"Programming_Language_Forecasting/#time-series-and-experimental-design","text":"Authors: Alana Willis, Clare Cruz, Daniel Nason, Megan Christy","title":"Time Series and Experimental Design"},{"location":"Programming_Language_Forecasting/#time-series-analysis-final-project","text":"The presentation can be found here , as well as the written report .","title":"Time Series Analysis Final Project"},{"location":"Programming_Language_Forecasting/#project-background","text":"","title":"Project Background"},{"location":"Programming_Language_Forecasting/#data","text":"The data can be found in this repo or on Kaggle .","title":"Data"},{"location":"Programming_Language_Forecasting/#findings","text":"In the dataset, python has grown the most in popularity of asking StackOverflow questions, followed by R and then various statistical and machine learning topics.","title":"Findings"},{"location":"Programming_Language_Forecasting/#sarima-models","text":"Model fits were chosen based on residual diagnostics. While both models fit the data relatively well and project a growing trend, additional modeling is needed to determine which variables are correlated with Python and R.","title":"SARIMA Models"},{"location":"Programming_Language_Forecasting/#var-models","text":"Since the predictor variables of interest show a strong linear correlation with R and Python, we fit a VAR model to attempt to understand these relationships. The models show a good fit to the variables, although slightly less of a good fit compared to the SARIMA models. Interestingly, the results show that the statistical and machine learning topics are not statistically significant predictors of Python, while only Classification and Cluster Analysis are statistically significant for R.","title":"VAR Models"},{"location":"Programming_Language_Forecasting/#conclusion","text":"","title":"Conclusion"},{"location":"Statistics_Genomics_High_Dimensional_Inference/","text":"Statistical Genomics and High Dimensional Inference High dimensional inference is a technique commonly used for studying biological data such as genomics due to the large number of features generated during research. These techniques help to extract information from the data by focusing on the features or combination of features that explain the most variance in the data, such as PCA or t-SNE. Course Project Authors: Anirban Chowdhury, Daniel Nason Links: - Code - Report This project evaluates the use of various statistical learning techniques on the Zeisel mice dataset. The goal is to better understand the performance of various algorithms on this data, both for comparing clusters generated from unsupervised learning models as well as multi-class classification on the categories found by the researchers in the Zeisel dataset. Data The raw data on the Zeisel mice can be found here . Unsupervised Learning PCA t-SNE Misclustering rate Misclustering rates by method and dimension reduction technique: Supervised Learning Misclassification rates by method and dimension reduction technique: Findings Due to the high dimensionality of the data, we employ multiple dimension reduction techniques before fitting our models to improve performance. We see that for the unsupervised learning models, hierarchical clustering using the t-SNE dimension reduction technique has the least disagreement of clusters compared to the clusters generated by the researchers. Additionally, for the supervised learning models, the logistic regression model built on data transformed using PCA minimizes classification error.","title":"Statistical Genomics and High Dimensional Inference"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#statistical-genomics-and-high-dimensional-inference","text":"High dimensional inference is a technique commonly used for studying biological data such as genomics due to the large number of features generated during research. These techniques help to extract information from the data by focusing on the features or combination of features that explain the most variance in the data, such as PCA or t-SNE.","title":"Statistical Genomics and High Dimensional Inference"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#course-project","text":"Authors: Anirban Chowdhury, Daniel Nason Links: - Code - Report This project evaluates the use of various statistical learning techniques on the Zeisel mice dataset. The goal is to better understand the performance of various algorithms on this data, both for comparing clusters generated from unsupervised learning models as well as multi-class classification on the categories found by the researchers in the Zeisel dataset.","title":"Course Project"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#data","text":"The raw data on the Zeisel mice can be found here .","title":"Data"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#unsupervised-learning","text":"","title":"Unsupervised Learning"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#pca","text":"","title":"PCA"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#t-sne","text":"","title":"t-SNE"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#misclustering-rate","text":"Misclustering rates by method and dimension reduction technique:","title":"Misclustering rate"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#supervised-learning","text":"Misclassification rates by method and dimension reduction technique:","title":"Supervised Learning"},{"location":"Statistics_Genomics_High_Dimensional_Inference/#findings","text":"Due to the high dimensionality of the data, we employ multiple dimension reduction techniques before fitting our models to improve performance. We see that for the unsupervised learning models, hierarchical clustering using the t-SNE dimension reduction technique has the least disagreement of clusters compared to the clusters generated by the researchers. Additionally, for the supervised learning models, the logistic regression model built on data transformed using PCA minimizes classification error.","title":"Findings"}]}