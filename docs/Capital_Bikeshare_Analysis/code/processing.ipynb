{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e85bdb",
   "metadata": {},
   "source": [
    "# data collection and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4822ae",
   "metadata": {},
   "source": [
    "## goals\n",
    "- collect data from the data sources of interest\n",
    "    - capital bikeshare: https://s3.amazonaws.com/capitalbikeshare-data/index.html\n",
    "    - station capacity: https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Bikes_Trails_WebMercator/MapServer/5/query?outFields=*&where=1%3D1&f=geojson\n",
    "    - weather: https://www.ncei.noaa.gov/access/services/data/v1\n",
    "- process data to calculate bike availability\n",
    "- combine data sources and engineer features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e0117-2900-460b-a170-69b9875c4ade",
   "metadata": {},
   "source": [
    "## Capital bikeshare\n",
    "\n",
    "Collect data from 2018-2019, as well as the end of 2017 to get a starting point for bike capacity at each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac67414-adb7-4186-9cc3-9e53f6f7370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db782260",
   "metadata": {},
   "outputs": [],
   "source": [
    "## string constants\n",
    "bike_share_url = 'https://s3.amazonaws.com/capitalbikeshare-data/index.html'\n",
    "header_xpath = '//*[@id=\"tbody-content\"]'\n",
    "pattern = r'^\\d{4}.*\\.csv$'\n",
    "new_pattern = r'^\\d{6}.*\\.csv$'\n",
    "old_pattern= r'^\\d{4}[A-Za-z]\\d.*\\.csv$'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c923d7f",
   "metadata": {},
   "source": [
    "### collecting the raw data\n",
    "\n",
    "The code is flexible enough to account for different years that the user can customize. Note that the fundamental structure of the data changes from March 2020 to May 2020; while the data collection code is unaffected by this, the processing code would require adjustments to manipulate this data into the expected format for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe262a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eacc79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\n",
    "    \"download.default_directory\": raw_data_path,  # Set download directory\n",
    "    \"download.prompt_for_download\": False,  # Disable download prompt\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"safebrowsing.enabled\": True,  # Disable safe browsing\n",
    "}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(bike_share_url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "zip_links = driver.find_element(By.XPATH, header_xpath)\n",
    "urls = []\n",
    "\n",
    "for z in zip_links.find_elements(By.TAG_NAME, 'a'):\n",
    "    urls.append(z.get_attribute('href'))\n",
    "\n",
    "for z in urls:\n",
    "    try:\n",
    "        result = int(z.split('/')[-1][:4])\n",
    "        if result >= 2017 and result < 2020:\n",
    "            driver.get(z)\n",
    "            time.sleep(5)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "time.sleep(30)\n",
    "driver.quit()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file_name in os.listdir(raw_data_path):\n",
    "    zip_path = os.path.join(os.getcwd(), 'raw_data', file_name)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if bool(re.match(pattern, file)):\n",
    "                zip_ref.extract(file, raw_data_path)\n",
    "        print(f'Extracted files from {zip_path}')\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f2eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bike_data(path):\n",
    "    data = pd.read_csv(\n",
    "        path,\n",
    "        usecols=['Bike number', 'Member type', 'Start date', 'End date', 'Start station number', 'End station number', 'Start station', 'End station'],\n",
    "        parse_dates=['Start date', 'End date'],\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31055436",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_raw_data = read_bike_data(os.path.join('raw_data', sorted([f for f in os.listdir(raw_data_path) if bool(re.match(old_pattern, f))], reverse=True)[0])).dropna(ignore_index=True)\n",
    "old_raw_data.columns = [\"_\".join(col.split(\" \")).lower() for col in old_raw_data.columns]\n",
    "raw_data = pd.concat([read_bike_data(os.path.join('raw_data', f)) for f in os.listdir(raw_data_path) if bool(re.match(new_pattern, f))], axis=0).dropna(ignore_index=True)\n",
    "raw_data.columns = [\"_\".join(col.split(\" \")).lower() for col in raw_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df0cfb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_rename_dict = {\n",
    "    'start_station_number' : 'station', \n",
    "    'end_station_number' : 'station',\n",
    "    'start_station' : 'name',\n",
    "    'end_station' : 'name'\n",
    "}\n",
    "stations = pd.concat(\n",
    "    [\n",
    "        raw_data[['start_station_number', 'start_station']].rename(columns=column_rename_dict),\n",
    "        raw_data[['end_station_number', 'end_station']].rename(columns=column_rename_dict)\n",
    "    ],\n",
    "    axis=0\n",
    ").drop_duplicates()\n",
    "stations_map = dict(zip(stations['station'], stations['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8ff84-6a6f-43b7-93fe-656ce622f267",
   "metadata": {},
   "source": [
    "### reshuffling\n",
    "\n",
    "Idea: bikes transition from one station to another without a user taking the trip (this is done by contractors who 'reshuffle' vans from one location to the next). To account for this, we need to add rows that account for this reshuffling in order to get a more accurate count of where bikes are and when they're being relocated in order to calculate availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8c66656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reshuffle_data(raw_data):\n",
    "\n",
    "    print('Filtering and sorting initial data...')\n",
    "    data = raw_data.copy().query('end_date > start_date')\n",
    "    data.sort_values(by=['bike_number', 'start_date'], inplace=True, ignore_index=True)\n",
    "\n",
    "    ## reshaping data\n",
    "    print('Reshaping data to long format...')\n",
    "    data['tmp_id'] = data.groupby('bike_number').cumcount()\n",
    "    data['begin'] = data['start_date'].astype(str) + ',' + data['start_station_number'].astype(str)\n",
    "    data['end'] = data['end_date'].astype(str) + ',' + data['end_station_number'].astype(str)\n",
    "    data_long = data.melt(id_vars=['tmp_id','bike_number'], value_vars=['begin', 'end'], var_name='event')\n",
    "    tmp = data_long.pop('value').str.split(',', expand=True).rename(columns={0:'timestamp', 1:'station'})\n",
    "    tmp['timestamp'] = pd.to_datetime(tmp['timestamp'])\n",
    "    tmp['station'] = tmp['station'].astype(int)\n",
    "    data_long = pd.concat([data_long, tmp], axis=1)\n",
    "\n",
    "    ## preparing data for reshuffling\n",
    "    print('Sorting data to identify reshuffling...')\n",
    "    data_long.sort_values(by=['bike_number', 'tmp_id', 'timestamp'], inplace=True, ignore_index=True)\n",
    "    data_long['next_station'] = data_long.groupby('bike_number')['station'].shift(-1)\n",
    "    data_long['previous_station'] = data_long.groupby('bike_number')['station'].shift(1)\n",
    "    data_long['reshuffle'] = False\n",
    "\n",
    "    ## identifying reshuffling\n",
    "    print('Identifying reshuffling...')\n",
    "    reshuffle_tmp = data_long.copy().query('(event==\"begin\" and previous_station.notnull() and station!=previous_station) or (event==\"end\" and next_station.notnull() and station!=next_station)')\n",
    "    reshuffle_tmp['event'] = np.where(reshuffle_tmp['event'] == 'end', 'begin', 'end')\n",
    "    reshuffle_tmp['reshuffle'] = True\n",
    "    reshuffle_tmp.rename(columns={'timestamp' : 'temp_timestamp'}, inplace=True)\n",
    "\n",
    "    ## generating fake timestamps for reshuffling\n",
    "    ## assumption: for gap in between riding events, reshuffling occurs\n",
    "        ## bike is reshuffled away from end station at 1/3 of time between end of ride and beginning of ride\n",
    "        ## bike is reshuffled to begin station at 2/3 of time between end of ride and beginning of ride\n",
    "    print('Processing reshuffling...')\n",
    "    reshuffle_tmp['next_timestamp'] = reshuffle_tmp.groupby('bike_number')['temp_timestamp'].shift(-1)\n",
    "    reshuffle_tmp['previous_timestamp'] = reshuffle_tmp.groupby('bike_number')['temp_timestamp'].shift(1)\n",
    "    reshuffle_tmp['calc_timestamp'] = np.where(reshuffle_tmp['event'] == 'begin', reshuffle_tmp['next_timestamp'], reshuffle_tmp['previous_timestamp'])\n",
    "    reshuffle_tmp['total_seconds_elapsed'] = (reshuffle_tmp['temp_timestamp'] - reshuffle_tmp['calc_timestamp']).dt.total_seconds().abs()\n",
    "    reshuffle_tmp['timestamp'] = np.where(\n",
    "        reshuffle_tmp['event'] == 'begin', \n",
    "        reshuffle_tmp['temp_timestamp'] + pd.to_timedelta(reshuffle_tmp['total_seconds_elapsed'] // 3, unit='s'),\n",
    "        reshuffle_tmp['temp_timestamp'] - pd.to_timedelta(reshuffle_tmp['total_seconds_elapsed'] // 3, unit='s')\n",
    "    )\n",
    "    \n",
    "\n",
    "    ## combining raw and reshuffled data\n",
    "    print('Combining raw and reshuffled data...')\n",
    "    reshuffle_cols = ['bike_number', 'event', 'timestamp', 'station', 'reshuffle']\n",
    "    data_long_comb = pd.concat([data_long[reshuffle_cols], reshuffle_tmp[reshuffle_cols]], axis=0)\n",
    "    data_long_comb.sort_values(by=['bike_number', 'timestamp'], inplace=True, ignore_index=True)\n",
    "    print('Done.')\n",
    "    return data_long_comb\n",
    "\n",
    "def generate_start_point(data):\n",
    "    start_point = data[['station']].drop_duplicates()\n",
    "    start_point['date'] = pd.to_datetime('2017-12-31')\n",
    "    start_point['hour'] = 23\n",
    "    last_arrival = data.iloc[data.groupby('bike_number')['timestamp'].idxmax(), :].query('event==\"end\"').groupby('station', as_index=False)['bike_number'].count().rename(columns={'bike_number' : 'arrivals'})\n",
    "    start_point = start_point.merge(last_arrival, on='station', how='left').fillna(0)\n",
    "    start_point['arrivals'] = start_point['arrivals'].astype(int)\n",
    "    start_point['departures'] = 0\n",
    "    return start_point\n",
    "\n",
    "def generate_aggregate_data(data, fill_na_value=0):\n",
    "    combined_columns = ['station', 'date', 'hour']\n",
    "    def agg_bike_data(data, begin=True, combined_columns=combined_columns):\n",
    "        query_filter = 'begin' if begin else 'end'\n",
    "        col_name = 'departures' if begin else 'arrivals'\n",
    "        agg_data = (\n",
    "            data.query(f\"event=='{query_filter}'\")\n",
    "            .assign(\n",
    "                date=lambda df: df['timestamp'].dt.date,\n",
    "                hour=lambda df: df['timestamp'].dt.hour\n",
    "            )\n",
    "            .groupby(combined_columns, as_index=False)['bike_number'].count()\n",
    "            .rename(columns={'bike_number' : col_name})\n",
    "        )\n",
    "        return agg_data\n",
    "    departures = agg_bike_data(data, begin=True)\n",
    "    arrivals = agg_bike_data(data, begin=False)\n",
    "    combined = pd.merge(departures, arrivals, how='outer', on=combined_columns).sort_values(by=combined_columns, ignore_index=True).fillna(0)\n",
    "    combined['date'] = pd.to_datetime(combined['date'])\n",
    "    return combined\n",
    "\n",
    "def process_data(data, start_point=False):\n",
    "    data = generate_reshuffle_data(data)\n",
    "    if start_point:\n",
    "        data = generate_start_point(data)\n",
    "    else:\n",
    "        data = generate_aggregate_data(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_data = process_data(old_raw_data, start_point=True)\n",
    "agg_data = process_data(raw_data, start_point=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6c4e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([start_data, agg_data], axis=0).sort_values(by=['station', 'date', 'hour'], ignore_index=True)\n",
    "combined_data['start_ind'] = combined_data['date'].dt.year == 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85523a9f",
   "metadata": {},
   "source": [
    "## capacity data\n",
    "\n",
    "To get relative capacity, need to get the size of the bike stations as well as their locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97da2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_url = 'https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Bikes_Trails_WebMercator/MapServer/5/query?outFields=*&where=1%3D1&f=geojson'\n",
    "response = requests.get(station_url)\n",
    "response_dict = response.json() # json.loads(response.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df8f802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_data(d):\n",
    "    return dict(\n",
    "        name=d['properties']['NAME'],\n",
    "        capacity=d['properties']['CAPACITY'],\n",
    "        latitude=d['properties']['LATITUDE'],\n",
    "        longitude=d['properties']['LONGITUDE']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "935a7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.DataFrame([extract_feature_data(d) for d in response_dict['features']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09a603",
   "metadata": {},
   "source": [
    "## weather data\n",
    "\n",
    "Get weather data information for the dates of interest at the daily level (finest grain of detail available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6661f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_url = 'https://www.ncei.noaa.gov/access/services/data/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9658d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(data=combined_data, date_format=\"%Y-%m-%d\"):\n",
    "    agg_data = data['date'].agg(['min', 'max']).to_dict()\n",
    "    results_dict = {\n",
    "        'startDate' : agg_data[\"min\"].strftime(date_format),\n",
    "        'endDate' : agg_data[\"max\"].strftime(date_format)\n",
    "    }\n",
    "    return results_dict\n",
    "def create_noaa_payload(\n",
    "        datatypes=['TAVG', 'TMAX', 'TMIN', 'PRCP', 'SNOW'], ## types of weather data\n",
    "        date_data=agg_data, dataset='daily-summaries', ## table to pull data from\n",
    "        stations='USW00013743', ## closest station to DC (Ronald Reagon Airport)\n",
    "        output_format='csv' ## desired output\n",
    "):\n",
    "    payload = {**get_date_range(date_data)}\n",
    "    payload['dataTypes'] = ','.join([c.upper() for c in datatypes])\n",
    "    payload['dataset'] = dataset\n",
    "    if isinstance(stations, str):\n",
    "        stations = [stations]\n",
    "    payload['stations'] = ','.join(stations)\n",
    "    payload['format'] = output_format\n",
    "    return payload\n",
    "def create_noaa_url(url=noaa_url, **kwargs):\n",
    "    payload = create_noaa_payload(**kwargs)\n",
    "    query_string = '&'.join([f'{k}={v}' for k, v in payload.items()])\n",
    "    query_url = f'{url}?{query_string}'\n",
    "    return query_url\n",
    "def process_weather_data(raw_string):\n",
    "    raw_result = iter(raw_string.text.strip().split('\\n'))\n",
    "    headers = next(raw_result)\n",
    "    headers = [h.replace('\"', '').strip().lower() for h in headers.split(',')[1:]]\n",
    "    result_lst = []\n",
    "    for row in raw_result:\n",
    "        row = iter(row.split(','))\n",
    "        next(row)\n",
    "        row_dict = {headers[i] : v.replace('\"', '').strip() for i, v in enumerate(row)}\n",
    "        result_lst.append(row_dict)\n",
    "    result_df = pd.DataFrame(result_lst)\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "    final_result = pd.concat([result_df['date'], result_df.select_dtypes(object).astype(int).div(10)], axis=1)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1601a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(create_noaa_url())\n",
    "weather_data = process_weather_data(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f304b0d",
   "metadata": {},
   "source": [
    "## holidays\n",
    "\n",
    "Add federal holidays to see if there is special behavior on these specific dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7bc20d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dates onto the dataframe - federal holidays\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start= combined_data['date'].min(), end=combined_data['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb811652",
   "metadata": {},
   "source": [
    "## combining all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d4a303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['name'] = combined_data['station'].map(stations_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e3ac3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = combined_data.merge(stations, on='name', how='inner').merge(weather_data, on='date').query(\"date<'2020-01-01'\")\n",
    "merged_data['holiday'] = merged_data['date'].isin(holidays).astype(int)\n",
    "merged_data.sort_values(['station', 'date', 'hour'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8963564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_parquet('../data/processed_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeee5cc",
   "metadata": {},
   "source": [
    "## removing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91dad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning up raw data folder.')\n",
    "for dirpath, subdirs, filenames in os.walk(raw_data_path):\n",
    "    for file in filenames:\n",
    "        os.remove(os.path.join(dirpath, file))\n",
    "        print('Removed {file}')\n",
    "    os.rmdir(dirpath)\n",
    "    print('Removed {dirpath}')\n",
    "print('Removed raw data folder and contents.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
